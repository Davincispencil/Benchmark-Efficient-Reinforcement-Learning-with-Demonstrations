11.24:
ddpg version code original actor and critic has only one dense layer, without hidden layer; adding one hidden layer makes it harder to converge to orginal goal, which is one reward point only.

对更复杂的reward, 像加两个penalty points的， 没有hidden layer的算法无法学出最优解，无论用哪种noise；而加一层hidden后，整个学习曲线非常平滑，似乎noise作用相对减弱，仍旧很难学到全局最优 

the shaking of its motion is caused by noise, larger noise range, larger shaking. The better action comes from its shaking range of motion.


inverse dynamics采用的action比原reacher采用的要大1000（screensize），故用predict做监督学习时要么做原预测再将预测结果乘1000做实际action; 要么直接将训练label action乘1000再去train,并将输出直接做action. 前者在用到ddpg中也要相应乘1000才能做实际action. newest version采用第二种，且对网络默认tanh输出做更改，扩大其输出范围，因为直接可用的action值有大于1的。
